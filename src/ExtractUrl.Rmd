---
title: "Extract URLs - Scrape Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r}
library(RSelenium)
library(tidyverse)
library(lubridate)
```

# Selenium setup

Start a selenium server and browser
```{r start-selenium}
port <- 4545L
rsDrv <- rsDriver(browser="firefox", port=port, verbose=F)

# Sleep to allow driver time to launch headless browser
Sys.sleep(2)
```

Grab  reference to remote client driver
```{r get-client-ref}
rsClient <- rsDrv[["client"]]
```

# Generate list of data to be processed

Create URLs for Global and USA data

```{r constants}
source("./Consts.R")
source("./FilePaths.R")
```

Create function to extract elements from target URL.

```{r find-elements-func}
#
# Find target elements to be processed.
#
# Parameters:
#   startUrl: starting URL
# Returns:
#   Elements to be processed in descending date order. Tibble with 3 columns containing: name, href, and raw_href.
#
FindTargetElements <- function(startUrl) {
    rsClient$navigate(startUrl)
    Sys.sleep(3)

    # Find most recent date that was processed
    global <- str_ends(startUrl, "_us", negate = TRUE)

    css_selector <- "a.js-navigation-open.Link--primary"
    elems <- rsClient$findElements(using = "css selector", 
                                   value = css_selector) 
    tbl_href <- tibble(name = character(), date = date, year = integer(), month = integer(), day = integer(), href = character(), raw_href = character())
    for (elem in elems) {
        name <- as.character(elem$getElementText())
        if (tolower(name) != "readme.md" && tolower(name) != ".gitignore") {
            sdate <- str_remove(name, ".csv")
            date <- as.Date(sdate, "%m-%d-%Y")
    
            year <- lubridate::year(date)
            month <- lubridate::month(date)
            day <- lubridate::day(date)

            href <- as.character(elem$getElementAttribute("href"))
            raw_href <- str_replace(href, "github.com", "raw.githubusercontent.com")
            raw_href <- str_replace(raw_href, "/blob", "")
            
            tbl_href <- tbl_href %>% add_row(name = name, date = date, year = year, month = month, day = day, href = href, raw_href = raw_href)
         }
    }
    tbl_href <- tbl_href %>%
                    arrange(desc(date))
    return (tbl_href)
}
```

Find elements.

```{r find-elements}
tbl_href <- FindTargetElements(URL.USA)
head(tbl_href, n = 5)
```

Stop selenium server

```{r stop-selenium}
rsDrv$server$stop()
```

Save URLs of processed data

```{r save-processed-hrefs}
full_path <- Output.GetHrefFilePath('href', global = FALSE)
write_csv(tbl_href, full_path, quote = 'needed')
```

Save current timestamp

```{r}
full_path <- Output.GetHrefFilePath('ts', global = FALSE)
ts <- format(today(), '%Y-%m-%d')
write_file(ts, full_path)

```

