---
title: "Use Spark for timeseries EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(lubridate)
library(sparklyr)
library(DBI)
library(assertive)
library(readr)
```

```{r}
source("SparkFunctions.R")
source("Consts.R")
source("FilePaths.R")
source("OurWorldInData_Common.R")
```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```

```{r load-ts-dataset}
df <- LoadDataset(verbose = TRUE)
df
```

Connect to local Spark standalone.

```{r connect-to-local-master}
# sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
# spark_web(sc)
```

```{r}
# owid_path <- GetOWIDPath()
# owid_spark_tbl <- spark_read_csv(sc, name = "OWID", path = owid_path, memory = FALSE)
# owid_spark_summ <- owid_spark_tbl |> group_by(location) |> summarize(count = n(),  mean = mean(total_deaths_per_million, na.rm = TRUE))
# dplyr::show_query(owid_spark_summ)
```

```{r}
# partitions <- owid_spark_tbl %>%
#   select(location, total_deaths_per_million, median_age, population_density) %>% 
#   sdf_random_split(training = 0.75, test = 0.25, seed = 1099)
```

```{r}
# Have not investigated using Leaflet directly with Spark (does not work on first try), we have create R dataframe
#system.time(df1 <-
#                sdf_collect(sdf))    # collect Spark data in R dataframe
```

Load plot libraries

```{r}
library(ggplot2)
library(plotly)

library(leaflet)
library(leaflet.extras)
library(leaflet.providers)
```

```{r}
df_latest <- Truncate(df, keep_type = 'last', is_global_data)
df_latest

```


```{r display-global-map}
m <- NULL
leaflet_map <- leaflet::leaflet(df_latest) %>%
    leaflet::addTiles() |>
    setView(lng = --2, lat = 54, zoom = 2.1) |>
    #addProviderTiles(providers$Stamen.Toner) |>
    #addProviderTiles(providers$CartoDB.Positron) |>
    addProviderTiles(providers$Esri.NatGeoWorldMap, options =providerTileOptions(opacity = 0.85)) |>
    #addProviderTiles(providers$Stamen.TonerLite) |>
    leaflet::addCircleMarkers(
        lng = ~ Longitude,
        lat = ~ Latitude,
        label = ~ paste0(continent, 
                         "/", 
                         location, 
                         ", Deaths: ", total_deaths,
                         ", Per Mil: ", total_deaths_per_million),
        color = 'red',
        radius = ~ scaled_var * 15
    ) |>
    leaflet.extras::addResetMapButton()
leaflet_map  # Print the map
```

```{r}
set.seed(1234)

```

```{r}
# df_spark_latest <- copy_to(sc, df_latest, overwrite = TRUE, MEMORY = TRUE)
# df_spark_latest <- df_spark_latest |> 
#     filter(! is.na(total_deaths_per_million)) |> 
#     filter(! is.na(human_development_index)) |> 
#     filter(! is.na(aged_65_older))
# 
# kmeans_model <- df_spark_latest |>
#     ml_kmeans(k = 6, 
#               features = c('total_deaths_per_million', 'aged_65_older'))
```

```{r}
# predicted <- ml_predict(kmeans_model, df_spark_latest) %>%
#   collect()
```

```{r}
# predicted %>%
#   ggplot(aes(total_deaths_per_million, aged_65_older)) +
#   geom_point(aes(total_deaths_per_million, aged_65_older, col = factor(prediction + 1)),
#     size = 2, alpha = 0.5
#   ) +
#   geom_point(
#     data = kmeans_model$centers, aes(total_deaths_per_million, aged_65_older),
#     col = scales::muted(c("red", "green", "blue", "yellow", "magenta", "orange")),
#     pch = "x", size = 12
#   ) +
#   scale_color_discrete(
#     name = "Predicted Cluster",
#     labels = paste("Cluster", 1:6)
#   ) +
#   labs(
#     x = "Total Deaths per Million",
#     y = "Aged 65 or Older",
#     title = "K-Means Clustering",
#     subtitle = "Use Spark.ML to predict cluster membership with the OWID dataset."
#   )
```

Disconnect from cluster

```{r}
# Spark.Disconnect(sc)
```

```{r}
#Spark.ShowJPS()
```

