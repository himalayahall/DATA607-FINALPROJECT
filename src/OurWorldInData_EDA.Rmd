---
title: "Use Spark for timeseries EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(lubridate)
library(sparklyr)
library(DBI)
library(assertive)
library(readr)
```

```{r}
source("SparkFunctions.R")
source("Consts.R")
source("FilePaths.R")
source("OurWorldInData_Common.R")
```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```


```{r load-ts-dataset}
df <- LoadDataset(verbose = TRUE)
df
```

Connect to local Spark standalone.

```{r connect-to-local-master}
# sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
# spark_web(sc)
```

```{r}
# sdf <- CopyToSpark(df)
# sdf
```

```{r}
# Have not investigated using Leaflet directly with Spark (does not work on first try), we have create R dataframe
#system.time(df1 <-
#                sdf_collect(sdf))    # collect Spark data in R dataframe
```


```{r}
```

Load plot libraries

```{r}
library(ggplot2)
library(plotly)

library(leaflet)
library(leaflet.extras)
library(leaflet.providers)
```

```{r}
df_latest <- Truncate(df, keep_type = 'last', is_global_data)
df_latest

```


```{r display-global-map}
m <- NULL
leaflet_map <- leaflet::leaflet(df_latest) %>%
    leaflet::addTiles() |>
    #setView(lng = --2, lat = 54, zoom = 2.1) |>
    #addProviderTiles(providers$Stamen.Toner) |>
    #addProviderTiles(providers$CartoDB.Positron) |>
    # addProviderTiles(providers$Esri.NatGeoWorldMap, options =      # providerTileOptions(opacity = 0.85)) |>
    #addProviderTiles(providers$Stamen.TonerLite) |>
    leaflet::addCircleMarkers(
        lng = ~ Longitude,
        lat = ~ Latitude,
        label = ~ paste0(continent, 
                         "/", 
                         location, 
                         ", Deaths: ", total_deaths,
                         ", Per Mil: ", total_deaths_per_million),
        color = 'red',
        radius = ~ scaled_var * 15
    ) |>
    leaflet.extras::addResetMapButton()
leaflet_map  # Print the map
```

```{r}
set.seed(1234)

```


```{r}
# plot data
rnd_locations <- sample(df$location, 10)

p <-
    df |> filter(location %in% rnd_locations) |>
    arrange(date) |>
    ggplot(aes(x = date, y = total_deaths_per_million, color = location)) +
    geom_line(size = 0.5) +
    scale_x_date(
        date_breaks = "1 year",
        date_minor_breaks = '1 month',
        date_labels = "%Y"
    ) +
    scale_y_continuous(limits = c(0, 5000),
                       labels = scales::label_comma()) +
    labs(
        title = "COVID Deaths Per Million",
        subtitle = "2020 - 2023",
        caption = "source: Our World In Data",
        y = "Deaths"
    ) +
    theme_minimal()
ggplotly(p)
```

```{r}
# plot data
locations <- c('Germany', 'United Kingdom', 'United States', 'Japan', 'Canada', 'India')

p <-
    df |> 
    filter(location %in% locations) |>
    filter(! is.na(excess_mortality)) |>
    ggplot(aes(x = date, y = excess_mortality_cumulative_per_million, color = location)) +
    geom_line(size = 0.5) +
    labs(
        title = "Excess mortality: Cumulative number of deaths from all causes compared to projection based on
previous years, per million people",
        subtitle = "2020 - 2023",
        caption = "source: Our World In Data"
    ) +
    theme_minimal()
ggplotly(p)
```


Bring in S&P 500 data

```{r}
library(yfR)

# set options for algorithm
my_ticker <- '^GSPC'
first_date <- "1950-01-01"
last_date <- Sys.Date()

# fetch data
df_yf <- yf_get(tickers = my_ticker, 
                first_date = first_date,
                last_date = last_date)

# output is a tibble with data
glimpse(df_yf)
```

```{r}
min_date <- min(df$date)

covid_df_yr <- df_yf |>
    filter(ref_date >= min_date)

glimpse(covid_df_yr)
```

```{r}
library(ggplot2)

p <- ggplot(covid_df_yr, aes(x = ref_date, y = price_adjusted)) + 
  geom_line() +
  labs(
    title = paste0("SP500 Index Value (", 
                   year(min(df_yf$ref_date)), ' - ',
                   year(max(df_yf$ref_date)), ")"
    ),
    x = "Time", 
    y = "Index Value",
    caption = "Data from Yahoo Finance <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

```{r}
group_df <- df |> filter(location == 'United States') 
group_covid_sp <- left_join(group_df, covid_df_yr, by = c('date' = 'ref_date'))
group_covid_sp
```

```{r}
set.seed(20220713)

n_tickers <- 10
df_sp500 <- yf_index_composition("SP500")
```

```{r}
rnd_tickers <- sample(df_sp500$ticker, n_tickers)

cat(paste0("The selected tickers are: ", 
           paste0(rnd_tickers, collapse = ", ")))
```

```{r}
df_yf <- yf_get(tickers = rnd_tickers,
                first_date = min_date,
                last_date = Sys.Date())
```

```{r}
library(ggplot2)

p <- ggplot(df_yf, 
            aes(x = ref_date, 
                y = cumret_adjusted_prices, 
                color = ticker)) + 
  geom_line() +
  labs(
    title = paste0("SP500 Index Value (", 
                   year(min(min_date)), ' - ',
                   year(max(df_yf$ref_date)), ")"
    ),
    x = "Time", 
    y = "Accumulated Return (from 100%)",
    caption = "Data from Yahoo Finance <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

```{r}

```

Disconnect from cluster

```{r}
# Spark.Disconnect(sc)
```

```{r}
# Spark.ShowJPS()
```

