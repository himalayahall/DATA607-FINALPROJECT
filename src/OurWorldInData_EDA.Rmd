---
title: "Use Spark for timeseries EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(lubridate)
library(sparklyr)
library(DBI)
library(assertive)
library(readr)
```

```{r}
source("SparkFunctions.R")
source("./Consts.R")
source("./FilePaths.R")
source("./OurWorldInData_CleanStatsTable.R")
```


```{r load-functions}
#
# Load data
#
# Parameters:
#   verbose: TRUE to print progress
LoadDataset <- function(verbose = TRUE) {
    file_name <-
        Output.GetOurWorldInDataFilePath()
    
    cwd <- getwd()
    
    file_path <-
        paste0(cwd, "/", file_name)
    df <- readr::read_csv(file = file_path, col_names = TRUE)

    lat_long <- LoadCountryLatLong()

    df <- left_join(df, lat_long, by = c('iso_code' = 'Alpha-3 code'))

    df <- df |>
        mutate(date = as.Date(date, format = '%m/%d/%y'))
    df$year = format(df$date, '%Y')
    df$month = format(df$date, '%m')
    df$day = format(df$date, '%d')
    
    df <- AddScaledDeaths('deaths', df)
    
    return (df)
}

LoadCountryLatLong <- function() {
    file_name <-
        Output.GetCountryLatLongFilePath()
    
    cwd <- getwd()
    
    file_path <-
        paste0(cwd, "/", file_name)
    df <- readr::read_csv(file = file_path, col_names = TRUE)
    
    df <- rename(df, Longitude = `Longitude (average)`)
    df <- rename(df, Latitude = `Latitude (average)`)
    
    return (df)
}

AddScaledDeaths <- function(ts_type, df) {
    # add scaled variable for plotting purposes
    if (ts_type == 'deaths') {
        max_tot_deaths_per_million <- max(df$total_deaths_per_million, na.rm = TRUE)
        df <- df |> 
            filter(! is.na(total_deaths_per_million)) |>
            mutate(scaled_var = total_deaths_per_million / max_tot_deaths_per_million)
    } else {
         max_confirmed <- max(df$confirmed)
        df <- df |> 
            mutate(scaled_var = confirmed / max_confirmed)
    }
    return (df)
}

#
# Truncate data. For each Country/Province (global) or Province/State  (USA) keep latest observation,
# or x monthly observations, or x yearly observations.
# 
# Parameters:
#   df: dataframe
# keep_type: truncation type c('last', 'month', 'year'). For month and year an optional count prefix
# may is allowed Example, '6 month' to keep semi-annual observations. First and last observations are
# always kep for month and year type.
#   global: TRUE if this is global data, FALSE for USA
Truncate <- function(df, keep_type = 'last', global) {
    
    n_days <- NULL
    if (str_detect(keep_type, 'month')) {
        n_days <- 30
        if (str_detect(keep_type, "[0-9]")) {
            count <- str_extract(keep_type, "[0-9]{1,2}")
            n_days <- n_days * as.integer(count)
        }
    } else {
        if (str_detect(keep_type, 'year')) {
            n_days <- 365
            if (str_detect(keep_type, "[0-9]")) {
                count <- str_extract(keep_type, "[0-9]{1,2}")
                n_days <- n_days * as.integer(count)
            }
        }
    }
    
    # keep only latest data? if so only keep last row
    if (keep_type == 'last') {
        df <-
            df |>
            group_by(location) |>
            filter(row_number() == n())
    } else if (str_detect(keep_type, 'month')) {
        df <-
            df |>
            group_by(location) |>
            filter(row_number() == 1 |
                       row_number() == n() |
                       row_number() %% n_days == 0)
    } else if (str_detect(keep_type, 'year')) {
        df <-
            df |>
            group_by(location) |>
            filter(row_number() == 1 |
                       row_number() == n() |
                       row_number() %% n_days == 0)
    }
    return (df)
}

# Copy dataframe to Spark.
#
# Parameters:
#   df: dataframe'
# Return:
#   Spark dataframe
#
CopyToSpark <- function(df) {
    sdf <- copy_to(
        sc,
        df,
        name = "coviddata",
        header = TRUE,
        memory = TRUE,
        overwrite = TRUE
    )
    sdf_dim(sdf)
    return (sdf)
}

```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```


```{r load-ts-dataset}
df <- LoadDataset(verbose = TRUE)
df
```

Connect to local Spark standalone.

```{r connect-to-local-master}
# sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
# spark_web(sc)
```

```{r}
# sdf <- CopyToSpark(df)
# sdf
```

```{r}
# Have not investigated using Leaflet directly with Spark (does not work on first try), we have create R dataframe
#system.time(df1 <-
#                sdf_collect(sdf))    # collect Spark data in R dataframe
```


```{r}
```

Load plot libraries

```{r}
library(ggplot2)
library(plotly)

library(leaflet)
library(leaflet.extras)
library(leaflet.providers)
```

```{r}
df_latest <- Truncate(df, keep_type = 'last', is_global_data)
df_latest
```


```{r display-global-map}
m <- NULL
leaflet_map <- leaflet::leaflet(df_latest) %>%
    #leaflet::addTiles() |>
    #addProviderTiles(providers$Stamen.Toner) |>
    #addProviderTiles(providers$CartoDB.Positron) |>
    # addProviderTiles(providers$Esri.NatGeoWorldMap, options = providerTileOptions(opacity = 0.85)) |>
    addProviderTiles(providers$Stamen.TonerLite) |>
    leaflet::addCircleMarkers(
        lng = ~ Longitude,
        lat = ~ Latitude,
        label = ~ paste0(continent, 
                         "/", 
                         location, 
                         ", Deaths: ", total_deaths,
                         ", Per Mil: ", total_deaths_per_million),
        color = 'red',
        radius = ~ scaled_var * 15
    ) |>
    leaflet.extras::addResetMapButton()
leaflet_map  # Print the map
```


```{r}
# plot data
p <-
    df |> filter(Country_Region %in% c('Germany', 'US', 'India', 'Russia')) |>
    arrange(date) |>
    ggplot(aes(x = date, y = deaths, color = Country_Region)) +
    geom_line(size = 0.5) +
    scale_x_date(
        date_breaks = "1 year",
        date_minor_breaks = '1 month',
        date_labels = "%Y"
    ) +
    scale_y_continuous(limits = c(0, 1000000),
                       labels = scales::label_comma()) +
    labs(
        title = "COVID Deaths",
        subtitle = "2020 - 2023",
        caption = "source: JHU",
        y = "Deaths"
    ) +
    theme_minimal()
ggplotly(p)
```

```{r}
# plot data
p <-
    df |> filter(Country_Region %in% c('Germany', 'US', 'India', 'Russia')) |>
    arrange(date) |>
    ggplot(aes(x = date, y = deaths_lagged, color = Country_Region)) +
    geom_line(size = 0.5) +
    scale_x_date(
        date_breaks = "1 year",
        date_minor_breaks = '1 month',
        date_labels = "%Y"
    ) +
    scale_y_log10() +
    #scale_y_continuous(limits = c(0, 20000),
    #                   labels = scales::label_comma()) +
    labs(
        title = "Daily COVID Deaths",
        subtitle = "2020 - 2023",
        caption = "source: JHU",
        y = "Deaths"
    ) +
    theme_minimal()
ggplotly(p)
```


```{r}
library(lubridate)
p <- df |> filter(Country_Region %in% c("US", 'India', 'Germany')) |>
    ggplot( aes(x = date, y = deaths_lagged, color = Country_Region)) + 
  geom_line() +
  labs(
    title = paste0("Daily COVID Deaths (", 
                   year(min(df$date)), ' - ',
                   year(max(df$date)), ")"
    ),
    x = "Time", 
    y = "Daily Deaths",
    caption = "Data from JHU <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

Bring in S&P 500 data

```{r}
library(yfR)

# set options for algorithm
my_ticker <- '^GSPC'
first_date <- "1950-01-01"
last_date <- Sys.Date()

# fetch data
df_yf <- yf_get(tickers = my_ticker, 
                first_date = first_date,
                last_date = last_date)

# output is a tibble with data
glimpse(df_yf)
```

```{r}
min_date <- min(df$date)

covid_df_yr <- df_yf |>
    filter(ref_date >= min_date)

glimpse(covid_df_yr)
```

```{r}
library(ggplot2)

p <- ggplot(covid_df_yr, aes(x = ref_date, y = price_adjusted)) + 
  geom_line() +
  labs(
    title = paste0("SP500 Index Value (", 
                   year(min(df_yf$ref_date)), ' - ',
                   year(max(df_yf$ref_date)), ")"
    ),
    x = "Time", 
    y = "Index Value",
    caption = "Data from Yahoo Finance <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

```{r}
group_df <- df |> filter(Country_Region == 'US')
group_covid_sp <- left_join(group_df, covid_df_yr, by = c('date' = 'ref_date'))
group_covid_sp
```

```{r}
lm(group_covid_sp$price_adjusted ~ group_covid_sp$deaths_lagged)
```

```{r}
set.seed(20220713)

n_tickers <- 10
df_sp500 <- yf_index_composition("SP500")
```

```{r}
rnd_tickers <- sample(df_sp500$ticker, n_tickers)

cat(paste0("The selected tickers are: ", 
           paste0(rnd_tickers, collapse = ", ")))
```

```{r}
df_yf <- yf_get(tickers = rnd_tickers,
                first_date = min_date,
                last_date = Sys.Date())
```

```{r}
library(ggplot2)

p <- ggplot(df_yf, 
            aes(x = ref_date, 
                y = cumret_adjusted_prices, 
                color = ticker)) + 
  geom_line() +
  labs(
    title = paste0("SP500 Index Value (", 
                   year(min(min_date)), ' - ',
                   year(max(df_yf$ref_date)), ")"
    ),
    x = "Time", 
    y = "Accumulated Return (from 100%)",
    caption = "Data from Yahoo Finance <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

```{r}

```

Disconnect from cluster

```{r}
# Spark.Disconnect(sc)
```

```{r}
# Spark.ShowJPS()
```

