---
title: "Use Spark for timeseries EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
library(assertive)
library(readr)
```

```{r}
source("SparkFunctions.R")
source("./Consts.R")
source("./FilePaths.R")
source("./DailyData_CleanStatsTable.R")
source("./TimeSeries_CleanStatsTable.R")

```


```{r load-functions}
#
# Load time-series data
#
# Parameters:
#   ts_type: c('deaths', 'confirmed')
#   truncate: TRUE to keep only latest data per Country/State
#   global: TRUE for global dataset, FALSE for USA (default)
#   verbose: TRUE to print progress
LoadTimeSeriesDataset <- function(ts_type, truncate = FALSE, global = FALSE, verbose = TRUE) {
    file_name <-
        Output.GetTimeSeriesFileName(ts_type = ts_type, global = global)
    
    cwd <- getwd()
    
    file_path <-
        paste0(cwd, "/../data/processed/timeseries/", file_name)
    df <- readr::read_csv(file = file_path, col_names = TRUE)
    
    # replace backslash chars (backslash not handles well by Spark)
    df <- rename(df, 'Province_State' = 'Province/State')
    df <- rename(df, 'Country_Region' = 'Country/Region')
    
    # tidy data - pivot longer on date columns
    df <- df |>
        pivot_longer(contains('/'), names_to = 'date', values_to = ts_type)
    
    # mutate date type
    df <- df |>
        mutate(date = as.Date(date, format = '%m/%d/%y'))
    
    # keep only latest data? if so only keep last row
    if (truncate) {
        df <- Truncate(df, global = global)
    }

    # add scaled variable for plotting purposes
    if (ts_type == 'deaths') {
        max_deaths <- max(df$deaths)
        df <- df |> 
            mutate(scaled_var = deaths / max_deaths)
    } else {
         max_confirmed <- max(df$confirmed)
        df <- df |> 
            mutate(scaled_var = confirmed / max_confirmed)
    }
    
    # sort by date
    df <- df |> arrange(date)
    return (df)
}


#
# Truncate data to keep only the latest observation.
# 
# Parameters:
#   df: dataframe
#   global: TRUE if this is global data, FALSE for USA
Truncate <- function(df, global) {
    # keep only latest data? if so only keep last row
    if (global) {
        # Global
        df <-
            df |>
            group_by(Country_Region) |> 
            filter(row_number() == n())
    } else
    {
        # USA
        df <-
            df |> 
            group_by(Province_State) |> 
            filter(row_number() == n())
    }
    return (df)
}

# Copy dataframe to Spark.
#
# Parameters:
#   df: dataframe'
# Return:
#   Spark dataframe
#
CopyToSpark <- function(df) {
    sdf <- copy_to(sc,
                   df,
                   name = "coviddata",
                   header = TRUE,
                   memory = TRUE,
                   overwrite = TRUE)
    sdf_dim(sdf)
    return (sdf)
}

```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```


```{r load-ts-dataset}
df <- LoadTimeSeriesDataset(ts_type = 'deaths', global = is_global_data, verbose = TRUE)
df
```

Connect to local Spark standalone.

```{r connect-to-local-master}
# sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
# spark_web(sc)
```

```{r}
# sdf <- CopyToSpark(df)
# sdf
```

```{r}
# Have not investigated using Leaflet directly with Spark (does not work on first try), we have create R dataframe
#system.time(df1 <-
#                sdf_collect(sdf))    # collect Spark data in R dataframe
```


```{r}
library(ggplot2)
library(plotly)
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
library(leaflet)
library(leaflet.extras)
library(leaflet.providers)
```

```{r}
truncated_df <- Truncate(df, is_global_data)
truncated_df
```


```{r display-global-map}

m <- NULL
if (is_global_data) {
    leaflet_map <- leaflet::leaflet(truncated_df) %>%
        #leaflet::addTiles() |>
        #addProviderTiles(providers$Stamen.Toner) |>
        #addProviderTiles(providers$CartoDB.Positron) |>
        #addProviderTiles(providers$Esri.NatGeoWorldMap, options = providerTileOptions(opacity = 0.85)) |>
        addProviderTiles(providers$Stamen.TonerLite) |>
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Country_Region, ", ", Province_State, " Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_var * 15
        ) |>
        leaflet.extras::addResetMapButton()
} else {
    leaflet_map <- leaflet::leaflet(df1) %>%
        leaflet::addProviderTiles("Stamen.Toner") %>%
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Province_State, ", Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_tot_deaths * 10
        ) |>
        leaflet.extras::addResetMapButton()
}
leaflet_map  # Print the map
```

```{r}

```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.ShowJPS()
```

