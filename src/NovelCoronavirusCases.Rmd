---
title: "Scrape Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r}
library(tidyverse)
```

```{r}
source("./Consts.R")
source("./FilePaths.R")
```

# Generate list of data to be processed

# Setup parallel processing

```{r}
library(doParallel)
library(parallel)
```

```{r par-time-series-scraper}
ScrapeTSStats.GetType <- function(targetUrl) {
    ts_type <- NULL
    if (str_detect(targetUrl, "deaths")) {
        ts_type <- 'deaths'
    }
    else {
        if (str_detect(targetUrl, "confirmed")) {
            ts_type <- 'confirmed'
        }
    }
    
    assert_is_not_null(ts_type)
    
    return (ts_type)
}

ScrapeTSStats <- function(targetUrl) {
    
    # need to load libraries on cluster nodes
    library(readr)

    stats_tbl <- readr::read_csv(file = targetUrl)
    
    return (stats_tbl)    
}

ScrapeTSStats.Global <- function(url_index) {
    
    source("./FilePaths.R")
    source("./CleanStatsTable.R")

     # need to load libraries on cluster nodes
    library(tidyverse)
    library(readr)
    
    #url_index <- (...)

   targetUrl <- pluck(tbl_href['raw_href'], 1, url_index)
   ts_tbl <- ScrapeTSStats(targetUrl = targetUrl)

   cleanedTbl <- CleanTSTable(ts_tbl)

   ts_type <- ScrapeTSStats.GetType(targetUrl)
   path <- Output.GetTimeSeriesFilePath(ts_type, global = TRUE)

   print(paste0(ts_type, ", input: ", targetUrl, ", output: ", path))
   write_csv(cleanedTbl, file = path, quote = 'needed')

    return (url_index)    
}

ScrapeTSStats.USA <- function(url_index) {
    
    source("./FilePaths.R")
    source("./CleanStatsTable.R")

        # need to load libraries on cluster nodes
    library(tidyverse)
    library(readr)
    
   # url_index <- (...)

    targetUrl <- pluck(tbl_href['raw_href'], 1, url_index)
    ts_tbl <- ScrapeTSStats(targetUrl = targetUrl)
    
    cleanedTbl <- CleanTSTable(ts_tbl)

    ts_type <- ScrapeTSStats.GetType(targetUrl)
    path <- Output.GetTimeSeriesFilePath(ts_type, global = FALSE)
    
   print(paste0(ts_type, ", input: ", targetUrl, ", output: ", path))
    write_csv(cleanedTbl, file = path, quote = 'needed')

    return (url_index)    
}

```

Create function to run on cluster

```{r par-daily-scrape-funcs}
#
# Scrape stats table
# Parameters:
#   targetUrl: URL to process
#
# Returns:
#   Scraped stats table
#
ScrapeStats <- function(targetUrl) {
    
    # need to load libraries on cluster nodes
    library(readr)

    stats_tbl <- readr::read_csv(file = targetUrl)
    
    return (stats_tbl)    
}

#
# Scrape Global stats table
# Parameters:
#   url_index: index of table to process
#
# Returns:
#   Scraped stats table
#
ScrapeStats.Global <- function(...) {
    
    library(purrr)

    source("./FilePaths.R")
    source("./CleanStatsTable.R")

    # get index of URL to process
    url_index <- (...)

    # get target URL and date from exported namespace
    targetUrl <- pluck(tbl_href['raw_href'], 1, url_index)   # tbl_href$raw_href[url_index]
    targetDate <- pluck(tbl_href['date'], 1, url_index)  # tbl_href$date[url_index]
    
    stats_tbl <- ScrapeStats(targetUrl)

    cleanedTbl <- CleanStatsTable.Global(statsTbl = stats_tbl)
    cleanedTbl <- AddDateComponents(statsTbl = cleanedTbl, targetDate = targetDate)
     
    path <- Output.GetDataFilePath(targetDate = targetDate, global = TRUE)
    write_csv(cleanedTbl, file = path, quote = 'needed')
     
    return (url_index)
}

#
# Scrape USA stats table
# Parameters:
#   url_index: index of table to process
#
# Returns:
#   Scraped stats table
#
ScrapeStats.USA <- function(...) {
    
    library(purrr)
    
    source("./FilePaths.R")
    source("./CleanStatsTable.R")
    
    # get index of URL to process
    url_index <- (...)
    
    # get target URL and date from exported namespace
    targetUrl <- pluck(tbl_href['raw_href'], 1, url_index)
    targetDate <- pluck(tbl_href['date'], 1, url_index)

    stats_tbl <- ScrapeStats(targetUrl)
    
    cleanedTbl <- CleanStatsTable.USA(statsTbl = stats_tbl)
    cleanedTbl <-
        AddDateComponents(statsTbl = cleanedTbl, targetDate = targetDate)
    
    path <-
        Output.GetDataFilePath(targetDate = targetDate, global = FALSE)
    write_csv(cleanedTbl, file = path, quote = 'needed')
    
    return (url_index)
}

```

Source local cluster functions.

```{r source-cluster-functions}
source("./LocalClusterFunctions.R")
```

Start cluster
```{r start-cluster}
cr <- Cluster.Start()
```
i
```{r}
targetURL <- URL.TS.GLOBAL.CONFIRMED
targetURL
```


Select which data to scrape: `URL.GLOBAL` or `URL.USA`
```{r set-target-url}
# targetURL <- #URL.GLOBAL URL.USA
# targetURL
```

```{r set-target-url}
is_global_data <- str_detect(targetURL, pattern = "_global")
is_global_data
```

```{r load-hrefs}
# library(lubridate)
#     
# 
# href_path <- Output.GetHrefFilePath('href', global = is_global_data)
# tbl_href <- read_csv(href_path)
# tbl_href <- tbl_href |> mutate(date = as.Date(date, "%m-%d-%Y"))
# tbl_href
```

```{r}
tbl_href <- tibble(raw_href = c(targetURL))
```

Export namespace to cluster nodes
```{r export-namespace}
# Function will be run on cluster nodes. Must export below namespaces
Cluster.ExportNamespace(cr, list('ScrapeStats', 'ScrapeStats.USA', 'ScrapeStats.Global', 'ScrapeTSStats', 'ScrapeTSStats.USA', 'ScrapeTSStats.Global', 'tbl_href'))
```

Generate sequence id for all URLs. Each work item - i.e. URL to be processed - will be accessed by it's index in `tbl_href` table.

```{r generate-url-id}

# Which URLs to process. For full set: start_href = 1, max_hrefs = NROW(tbl_href)
start_href <- 1
max_href <- NROW(tbl_href)

# For dev/testing purposes create a small sequence. For example, a sequence of 1:5 will process first 5 URLs
# For the final run use seq_along(1:NROW(tbl_href))
seq_id <- seq(from = start_href, length.out = max_href)
glimpse(seq_id)
```

Process URLs using cluster (map/reduce)
```{r par-process}
work_results <- NULL
if (is_global_data) {
    system.time(work_results <-
                    Cluster.ParApply(cr, seq_id, ScrapeTSStats.Global))
} else {
    system.time(work_results <-
                    Cluster.ParApply(cr, seq_id, ScrapeTSStats.USA))
}

# 
# if (is_global_data) {
#     system.time(work_results <-
#                     Cluster.ParApply(cr, seq_id, ScrapeStats.Global))
# } else {
#     system.time(work_results <-
#                     Cluster.ParApply(cr, seq_id, ScrapeStats.USA))
# }

unlist(work_results)
```

Stop  cluster
```{r stop-cluster}
Cluster.Stop(cr)

```


```{r echo=FALSE aggregate-results}
#cum_results <- bind_rows(work_results)

# sort by descending date order
#cum_results <- cum_results %>% arrange(desc(Date))

#glimpse(cum_results)
```


```{r echo = FALSE save-processed-hrefs}
# save_set <- tbl_href[start_href:max_href, ]
# full_path <- Output.GetFullPath('href', global = FALSE)
# write_csv(save_set, full_path, quote = 'needed')
```

```{r echo=FALSE save-processed-data}
# full_path <- Output.GetFullPath('data', global = FALSE)
# write_csv(cum_results, full_path, quote = 'needed')
```

```{r echo=FALSE save-timestamp}
# full_path <- Output.GetFullPath('ts', global = FALSE)
# d <- format(date(), '%Y-%m-%d')
# write_file(s, full_path)

```

