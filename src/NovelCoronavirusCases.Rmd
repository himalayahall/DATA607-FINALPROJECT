---
title: "Scrape Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r}
library(RSelenium)
library(tidyverse)
library(rvest)
#library(lubridate)
```

# Selenium setup

Start a selenium server and browser
```{r start-selenium}
port <- 4545L
rsDrv <- rsDriver(browser="firefox", port=port, verbose=F)

# Sleep to allow driver time to launch headless browser
Sys.sleep(5)
```

Grab  reference to remote client driver
```{r get-client-ref}
rsClient <- rsDrv[["client"]]
```

# Generate list of data to be processed

Create URLs for Global and USA data

```{r navigate-to-base-url}
urlUSA <- "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us"
urlGlobal <- "https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports"
```

Create function to extract elements from target URL.

```{r find-elements-func}
FindElements <- function(targetUrl) {
    rsClient$navigate(targetUrl)
    Sys.sleep(3)
    css_selector <- "a.js-navigation-open.Link--primary"
    elems <- rsClient$findElements(using = "css selector", 
                                   value = css_selector)    
    return (elems)
}
```

Find elements.

```{r find-elements}
elems <- FindElements(urlUSA)
```

Create tibble with 2 columns - name of CSV file name and HREF.

```{r create-dataframe-from-elements}
tbl_href <- tibble(name = character(), href = character(), raw_href = character())
for (elem in elems) {
    name <- as.character(elem$getElementText())
    if (tolower(name) != "readme.md" && tolower(name) != ".gitignore") {
        href <- as.character(elem$getElementAttribute("href"))
        raw_href <- str_replace(href, "github.com", "raw.githubusercontent.com")
        raw_href <- str_replace(raw_href, "/blob", "")
        tbl_href <- tbl_href %>% add_row(name = name, href = href, raw_href = raw_href)
    }
}
head(tbl_href, n = 5)
```

Stop selenium server.
```{r stop-selenium}
rsDrv$server$stop()
```


Add date column.

```{r add-date-col}
tbl_href <- tbl_href %>% 
    mutate(sdate = str_remove(name, ".csv"), 
           date = as.Date(sdate, "%m-%d-%Y")) %>%
    select(-sdate)

head(tbl_href, n = 5)
```

# Setup parallel processing

```{r}
library(doParallel)
library(parallel)
```

Create function that runs on cluster and processes one data item (URL).

```{r par-table-funcs}
CleanStatsTable <- function(statsTbl, targetDate) {
    library(tidyverse)
    library(lubridate)
    
    cleanedTbl <- statsTbl %>%
        mutate(
            Confirmed = as.integer(Confirmed),
            Deaths = as.integer(Deaths),
            Recovered = as.integer(Recovered),
            Active = as.integer(Active),
            FIPS = as.numeric(FIPS),
            Incident_Rate = as.numeric(Incident_Rate),
            Case_Fatality_Ratio = as.numeric(Case_Fatality_Ratio),
            
            Last_Update = as_datetime(Last_Update)
        )
    
    target_year <- lubridate::year(targetDate)
    target_month <- lubridate::month(targetDate)
    target_day <- lubridate::day(targetDate)

    cleanedTbl$Year = target_year
    cleanedTbl$Month = target_month
    cleanedTbl$Day = target_day

    return (cleanedTbl)
}

CleanStatsTable.USA <- function(statsTbl, targetDate) {
    library(tidyverse)
    library(lubridate)

    cleanedTbl <- CleanStatsTable(statsTbl = statsTbl, targetDate = targetDate)
    cleanedTbl <- cleanedTbl %>%
        mutate(
               People_Hospitalized = as.integer(People_Hospitalized),
               UID = as.integer(UID),

               Date = as.Date(Date),

               People_Tested = as.integer(People_Tested),
               Hospitalization_rate = as.numeric(Hospitalization_Rate),
               Mortality_Rate = as.numeric(Mortality_Rate),
               Testing_Rate = as.numeric(Testing_Rate),
               Total_Test_Results = as.integer(Total_Test_Results)
        )
    return (cleanedTbl)
}

CleanStatsTable.Global <- function(statsTbl, targetDate) {
    library(tidyverse)
    library(lubridate)

    cleanedTbl <- CleanStatsTable(statsTbl = stats_tbl, targetDate = targetDate)
    return (cleanedTbl)    
}

```


```{r par-scrape-funcs}
ScrapeStats.Global <- function(...) {
    
    # need to load libraries on cluster nodes
    library(tidyverse)
    library(readr)

    # get index of URL to process
    url_index <- (...)
    
    # get URL from exported namespace
    targetUrl <- tbl_href$raw_href[url_index]
    targetDate <- tbl_href$date[url_index]
    
    stats_tbl <- readr::read_csv(file = targetUrl)
    
    cleanedTbl <- CleanStatsTable.Global(statsTbl = stats_tbl, targetDate = targetDate)
    return (cleanedTbl)
}

ScrapeStats.USA <- function(...) {
    
    # need to load libraries on cluster nodes
    library(tidyverse)
    library(readr)

    # get index of URL to process
    url_index <- (...)
    
    # get URL from exported namespace
    targetUrl <- tbl_href$raw_href[url_index]
    targetDate <- tbl_href$date[url_index]

    stats_tbl <- readr::read_csv(file = targetUrl)
    
    cleanedTbl <- CleanStatsTable.USA(statsTbl = stats_tbl, targetDate = targetDate)
    return (cleanedTbl)
}

```

Source local cluster functions.

```{r souce-cluster-functions}
source("./LocalClusterFunctions.R")
```

Start cluster
```{r start-cluster}
cr <- Cluster.Start()
```

Export namespace to cluster nodes
```{r export-namespace}
# Function ScrapeStats.USA will be run on cluster nodes. It will access to tbl_href
Cluster.ExportNamespace(cr, list('ScrapeStats.USA', 'ScrapeStats.Global', 'CleanStatsTable', 'CleanStatsTable.USA', 'CleanStatsTable.Global', 'tbl_href'))
```

Generate sequence id for all URLs. Each work item - i.e. URL to be processed - will be accessed by it's index in `tbl_href` table.

```{r generate-url-id}

# Which URLs to process. For full set: start_href = 1, max_hrefs = NROW(tbl_href)
start_href <- 1
max_href <- 5

# For dev/testing purposes create a small sequence. For example, a sequence of 1:5 will process first 5 URLs
# For the final run use seq_along(1:NROW(tbl_href))
seq_id <- seq(from = start_href, length.out = max_href)
glimpse(seq_id)
```

Process URLs using cluster (map/reduce)
```{r par-process}
work_results <- Cluster.ParApply(cr, seq_id, ScrapeStats.USA)
```


Stop  cluster

```{r stop-cluster}
Cluster.Stop(cr)

```

Combine nested results 

```{r aggregate-results}
cum_results <- bind_rows(work_results)
glimpse(cum_results)
```


Save URLs of processed data

```{r save-processed-hrefs}
save_set <- tbl_href[start_href:max_href, ]
write_csv(save_set, paste0("../data/processed/covid_href_usa_", start_href, "_", start_href + max_href - 1, ".csv"), quote = 'needed')
```

Save COVID data
```{r save-processed-data}
write_csv(cum_results, paste0("../data/processed/covid_data_usa_", start_href, "_", start_href + max_href - 1, ".csv"), quote = 'needed')
```

