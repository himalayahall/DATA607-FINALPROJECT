---
title: "Scrape Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r}
library(RSelenium)
library(tidyverse)
library(rvest)
library(lubridate)
```

# Selenium setup

Start a selenium server and browser
```{r start-selenium}
port <- 4545L
rsDrv <- rsDriver(browser="firefox", port=port, verbose=F)

# Sleep to allow driver time to launch headless browser
Sys.sleep(5)
```

Grab  reference to remote client driver
```{r get-client-ref}
rsClient <- rsDrv[["client"]]
```

# Generate list of data to be processed

Navigate to base URL.

```{r navigate-to-base-url}
rsClient$navigate("https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us")
Sys.sleep(2)
```

Find elements.

```{r find-elements}
css_selector <- "a.js-navigation-open.Link--primary"
elems <- rsClient$findElements(using = "css selector", 
                               value = css_selector)
```

Create tibble with 2 columns - name of CSV file name and HREF.

```{r create-dataframe}
tbl_href <- tibble(name = character(), href = character())
for (elem in elems) {
    name <- as.character(elem$getElementText())
    if (tolower(name) != "readme.md") {
        href = as.character(elem$getElementAttribute("href"))
        tbl_href <- tbl_href %>% add_row(name = name, href = href)
    }
}
head(tbl_href, n = 5)
```

Stop selenium server.
```{r stop-selenium}
rsDrv$server$stop()
```


Add date column.

```{r add-date-col}
tbl_href <- tbl_href %>% 
    mutate(sdate = str_remove(name, ".csv"), 
           date = as.Date(sdate, "%m-%d-%Y")) %>%
    select(-sdate)

head(tbl_href, n = 5)
```

```{r}
# tbl_href <- tbl_href %>% 
#     mutate(year = lubridate::year(date),
#            month = lubridate::month(date), 
#            day = lubridate::day(date)) %>%
#     mutate(year = as.integer(year),
#            month = as.integer(month),
#            day = as.integer(day))
# 
# head(tbl_href, n = 5)
```

# Setup parallel processing

Create function that runs on cluster and processes one data item (URL).

```{r par-create-usa-stats-func}

ScrapeStats_USA <- function(...) {
    
    # need to load libraries on cluster nodes
    library(tidyverse)
    library(rvest)
    library(lubridate)
    
    # get index of URL to process
    scraped_url_index <- (...)
    
    usa_stats_css_selector <- ".js-csv-data"
    
    # get URL from exported namespace
    targetUrl <- tbl_href$href[scraped_url_index]
    
    # create traget URL
    if (! str_starts(targetUrl, "https:")) {
        if (str_starts(targetUrl, "/")) {
            targetUrl <- paste0("https://github.com", targetUrl)
        }
        else {
            targetUrl <- paste0("https://github.com/", targetUrl)
        }
    }
    
    # read from target URL, grab table data
    stats_table <- read_html(targetUrl) %>% 
        html_element(css = usa_stats_css_selector) %>%
        html_table()
    
    # first row contains column names
    first_row <- stats_table[1, ]
    colnames(stats_table) <- first_row[1,]
    
    # 
    stats_table <- stats_table %>% 
        select(-'NA') %>%    # first col is named 'NA', remove it
        slice(-1) %>%        # first row contains column names, remove it
        transmute(Confirmed = as.integer(Confirmed),
               Deaths = as.integer(Deaths),
               Recovered = as.integer(Recovered),
               Active = as.integer(Active),
               FIPS = as.numeric(FIPS),
               People_Hospitalized = as.integer(People_Hospitalized),
               Case_Fatality_Ratio = as.numeric(Case_Fatality_Ratio),
               UID = as.integer(UID),
               
               Date = as.Date(Date),
               Last_Update = as_datetime(Last_Update),
               
               People_Tested = as.integer(People_Tested),
               
               Hospitalization_rate = as.numeric(Hospitalization_Rate),
               Mortality_Rate = as.numeric(Mortality_Rate),
               Testing_Rate = as.numeric(Testing_Rate),
               Incident_Rate = as.numeric(Incident_Rate),
               Total_Test_Results = as.integer(Total_Test_Results),
        )
    
    return (stats_table)
}
```

Source local cluster functions.

```{r souce-cluster-functions}
source("./LocalClusterFunctions.R")
```

Start cluster
```{r start-cluster}
cr <- Cluster.Start()
```

Export namespace to cluster nodes
```{r export-namespace}
# Function ScrapeStats_USA will be run on cluster nodes. It will access to tbl_href
Cluster.ExportNamespace(cr, list('ScrapeStats_USA', 'tbl_href'))
```

Generate sequence id for all URLs. Each work item - i.e. URL to be processed - will be accessed by it's index in `tbl_href` table.

```{r generate-url-id}

# Which URLs to process. For full set: start_href = 1, max_hrefs = NROW(tbl_href)
start_href <- 1
max_href <- 5

# For dev/testing purposes create a small sequence. For example, a sequence of 1:5 will process first 5 URLs
# For the final run use seq_along(1:NROW(tbl_href))
seq_id <- seq(from = start_href, length.out = max_hrefs)
glimpse(seq_id)
```

Process URLs using cluster (map/reduce)
```{r par-process}
work_result <- Cluster.ParApply(cr, seq_id, ScrapeStats_USA)
```

Stop local cluster

```{r}
Cluster.Stop(cr)

```

Combine nested results 

```{r}
cum_results <- bind_rows(work_results)
glimpse(cum_results)
```

Look at dataset.
```{r}
glimpse(cum_results)
```


```{r save-processed-hrefs}
save_set <- tbl_href[start_href:max_href, ]
write_csv(save_set, paste0("./SOURCE/covid_href_", start_href, "_", start_href + max_href - 1, ".csv"), quote = 'needed')
```

```{r save-processed-data}
write_csv(cum_results, paste0("./SOURCE/covid_data_", start_href, "_", start_href + max_href - 1, ".csv"), quote = 'needed')
```

