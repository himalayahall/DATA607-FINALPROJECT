---
title: "Use Spark for timeseries EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
#library(lubridate)
library(sparklyr)
library(DBI)
library(assertive)
library(readr)
```

```{r}
source("SparkFunctions.R")
source("Consts.R")
source("FilePaths.R")

source("DailyData_CleanStatsTable.R")
source("TimeSeries_CleanStatsTable.R")

```


```{r load-functions}
#
# Load time-series data
#
# Parameters:
#   ts_type: c('deaths', 'confirmed')
#   global: TRUE for global dataset, FALSE for USA (default)
#   verbose: TRUE to print progress
LoadTimeSeriesDataset <- function(ts_type, global = FALSE, verbose = TRUE) {
    file_name <-
        Output.GetTimeSeriesFileName(ts_type = ts_type, global = global)
    
    cwd <- getwd()
    
    file_path <-
        paste0(cwd, "/../data/processed/timeseries/", file_name)
    df <- readr::read_csv(file = file_path, col_names = TRUE)
    
    df <- Tidy_Longer(ts_type, global, df)
    
    df <- AddScaledDeaths(ts_type = ts_type, df = df)

    # sort by date
    df <- df |> arrange(date)
    return (df)
}

Tidy_Longer <- function(ts_type, global, df) {
    
    # replace backslash chars (backslash not handles well by Spark)
    df <- rename(df, 'Province_State' = 'Province/State')
    df <- rename(df, 'Country_Region' = 'Country/Region')
    
    # tidy data - pivot longer on date columns
    df <- df |>
        pivot_longer(contains('/'), names_to = 'date', values_to = ts_type)
    
    df <- df |>
        mutate(date = as.Date(date, format = '%m/%d/%y')) 
    df$year = format(df$date, '%Y')
    df$month = format(df$date, '%m')
    df$day = format(df$date, '%d')

    if (global) {
        df <- df |> 
            mutate(prev = lag(deaths, order_by = Country_Region)) |> 
            mutate(deaths_lagged = ifelse(deaths > prev, deaths - prev, NA)) |> 
            select(-prev)
    } else {
        df <- df |> 
            mutate(prev = lag(deaths, order_by = Province_State)) |> 
            mutate(deaths_lagged = ifelse(deaths > prev, deaths - prev, NA)) |> 
            select(-prev)
    }
    
    return (df)    
}

AddScaledDeaths <- function(ts_type, df) {
    # add scaled variable for plotting purposes
    if (ts_type == 'deaths') {
        max_deaths <- max(df$deaths)
        df <- df |> 
            mutate(scaled_var = deaths / max_deaths)
    } else {
         max_confirmed <- max(df$confirmed)
        df <- df |> 
            mutate(scaled_var = confirmed / max_confirmed)
    }
    return (df)
}

#
# Truncate data. For each Country/Province (global) or Province/State  (USA) keep latest observation,
# or x monthly observations, or x yearly observations.
# 
# Parameters:
#   df: dataframe
# keep_type: truncation type c('last', 'month', 'year'). For month and year an optional count prefix
# may is allowed Example, '6 month' to keep semi-annual observations. First and last observations are
# always kep for month and year type.
#   global: TRUE if this is global data, FALSE for USA
Truncate <- function(df, keep_type = 'last', global) {
    
    n_days <- NULL
    if (str_detect(keep_type, 'month')) {
        n_days <- 30
        if (str_detect(keep_type, "[0-9]")) {
            count <- str_extract(keep_type, "[0-9]{1,2}")
            n_days <- n_days * as.integer(count)
        }
    } else {
        if (str_detect(keep_type, 'year')) {
            n_days <- 365
            if (str_detect(keep_type, "[0-9]")) {
                count <- str_extract(keep_type, "[0-9]{1,2}")
                n_days <- n_days * as.integer(count)
            }
        }
    }
    
    # keep only latest data? if so only keep last row
    if (global) {
        # Global
        if (keep_type == 'last') {
            df <-
                df |>
                group_by(Country_Region) |>
                filter(row_number() == n())
        } else if (str_detect(keep_type, 'month')) {
            df <-
                df |>
                group_by(Country_Region) |>
                filter(row_number() == 1 |
                           row_number() == n() |
                           row_number() %% n_days == 0)
        } else if (str_detect(keep_type, 'year')) {
            df <-
                df |>
                group_by(Country_Region) |>
                filter(row_number() == 1 |
                           row_number() == n() |
                           row_number() %% n_days == 0)
        }
    } else
    {
        # USA
        if (keep_type == 'last') {
            df <-
                df |>
                group_by(Province_State) |>
                filter(row_number() == n())
        } else if (keep_type == 'month') {
            df <-
                df |>
                group_by(Province_State) |>
                filter(row_number() == 1 |
                           row_number() == n() |
                           row_number() %% 30 == 0)
        } else if (keep_type == 'year') {
            df <-
                df |>
                group_by(Province_State) |>
                filter(row_number() == 1 |
                           row_number() == n() |
                           row_number() %% 365 == 0)
        }
    }
    return (df)
}

# Copy dataframe to Spark.
#
# Parameters:
#   df: dataframe'
# Return:
#   Spark dataframe
#
CopyToSpark <- function(df) {
    sdf <- copy_to(
        sc,
        df,
        name = "coviddata",
        header = TRUE,
        memory = TRUE,
        overwrite = TRUE
    )
    sdf_dim(sdf)
    return (sdf)
}

```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```


```{r load-ts-dataset}
df <- LoadTimeSeriesDataset(ts_type = 'deaths', global = is_global_data, verbose = TRUE)
df
```

Connect to local Spark standalone.

```{r connect-to-local-master}
# sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
# spark_web(sc)
```

```{r}
# sdf <- CopyToSpark(df)
# sdf
```

```{r}
# Have not investigated using Leaflet directly with Spark (does not work on first try), we have create R dataframe
#system.time(df1 <-
#                sdf_collect(sdf))    # collect Spark data in R dataframe
```


```{r}
```

Load plot libraries

```{r}
library(ggplot2)
library(plotly)

library(leaflet)
library(leaflet.extras)
library(leaflet.providers)
```


```{r display-global-map}
df_latest <- Truncate(df, keep_type = 'last', is_global_data)
df_latest

m <- NULL
if (is_global_data) {
    leaflet_map <- leaflet::leaflet(df_latest) %>%
        #leaflet::addTiles() |>
        #addProviderTiles(providers$Stamen.Toner) |>
        #addProviderTiles(providers$CartoDB.Positron) |>
        # addProviderTiles(providers$Esri.NatGeoWorldMap, options = providerTileOptions(opacity = 0.85)) |>
        addProviderTiles(providers$Stamen.TonerLite) |>
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Country_Region, ", ", Province_State, " Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_var * 15
        ) |>
        leaflet.extras::addResetMapButton()
} else {
    leaflet_map <- leaflet::leaflet(df_truncated) %>%
        leaflet::addProviderTiles("Stamen.Toner") %>%
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Province_State, ", Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_tot_deaths * 10
        ) |>
        leaflet.extras::addResetMapButton()
}
leaflet_map  # Print the map
```

```{r}
# plot data
p <-
    df |> filter(Country_Region %in% c('Germany', 'US', 'India', 'Russia')) |>
    arrange(date) |>
    ggplot(aes(x = date, y = deaths, color = Country_Region)) +
    geom_line(size = 0.5) +
    scale_x_date(
        date_breaks = "1 year",
        date_minor_breaks = '1 month',
        date_labels = "%Y"
    ) +
    scale_y_continuous(limits = c(0, 1000000),
                       labels = scales::label_comma()) +
    labs(
        title = "COVID Deaths",
        subtitle = "2020 - 2023",
        caption = "source: JHU",
        y = "Deaths"
    ) +
    theme_minimal()
ggplotly(p)
```

```{r}
# plot data
p <-
    df |> filter(Country_Region %in% c('Germany', 'US', 'India', 'Russia')) |>
    arrange(date) |>
    ggplot(aes(x = date, y = deaths_lagged, color = Country_Region)) +
    geom_line(size = 0.5) +
    scale_x_date(
        date_breaks = "1 year",
        date_minor_breaks = '1 month',
        date_labels = "%Y"
    ) +
    scale_y_log10() +
    #scale_y_continuous(limits = c(0, 20000),
    #                   labels = scales::label_comma()) +
    labs(
        title = "Daily COVID Deaths",
        subtitle = "2020 - 2023",
        caption = "source: JHU",
        y = "Deaths"
    ) +
    theme_minimal()
ggplotly(p)
```


```{r}
library(lubridate)
p <- df |> filter(Country_Region %in% c("US", 'India', 'Germany')) |>
    ggplot( aes(x = date, y = deaths_lagged, color = Country_Region)) + 
  geom_line() +
  labs(
    title = paste0("Daily COVID Deaths (", 
                   year(min(df$date)), ' - ',
                   year(max(df$date)), ")"
    ),
    x = "Time", 
    y = "Daily Deaths",
    caption = "Data from JHU <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

Bring in S&P 500 data

```{r}
library(yfR)

# set options for algorithm
my_ticker <- '^GSPC'
first_date <- "1950-01-01"
last_date <- Sys.Date()

# fetch data
df_yf <- yf_get(tickers = my_ticker, 
                first_date = first_date,
                last_date = last_date)

# output is a tibble with data
glimpse(df_yf)
```

```{r}
min_date <- min(df$date)

covid_df_yr <- df_yf |>
    filter(ref_date >= min_date)

glimpse(covid_df_yr)
```

```{r}
library(ggplot2)

p <- ggplot(covid_df_yr, aes(x = ref_date, y = price_adjusted)) + 
  geom_line() +
  labs(
    title = paste0("SP500 Index Value (", 
                   year(min(df_yf$ref_date)), ' - ',
                   year(max(df_yf$ref_date)), ")"
    ),
    x = "Time", 
    y = "Index Value",
    caption = "Data from Yahoo Finance <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

```{r}
group_df <- df |> filter(Country_Region == 'US')
group_covid_sp <- left_join(group_df, covid_df_yr, by = c('date' = 'ref_date'))
group_covid_sp
```

```{r}
lm(group_covid_sp$price_adjusted ~ group_covid_sp$deaths_lagged)
```

```{r}
set.seed(20220713)

n_tickers <- 10
df_sp500 <- yf_index_composition("SP500")
```

```{r}
rnd_tickers <- sample(df_sp500$ticker, n_tickers)

cat(paste0("The selected tickers are: ", 
           paste0(rnd_tickers, collapse = ", ")))
```

```{r}
df_yf <- yf_get(tickers = rnd_tickers,
                first_date = min_date,
                last_date = Sys.Date())
```

```{r}
library(ggplot2)

p <- ggplot(df_yf, 
            aes(x = ref_date, 
                y = cumret_adjusted_prices, 
                color = ticker)) + 
  geom_line() +
  labs(
    title = paste0("SP500 Index Value (", 
                   year(min(min_date)), ' - ',
                   year(max(df_yf$ref_date)), ")"
    ),
    x = "Time", 
    y = "Accumulated Return (from 100%)",
    caption = "Data from Yahoo Finance <https://finance.yahoo.com/>") + 
  theme_light() + 
  scale_y_log10() 

p
```

```{r}

```

Disconnect from cluster

```{r}
# Spark.Disconnect(sc)
```

```{r}
# Spark.ShowJPS()
```

