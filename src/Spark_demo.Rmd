---
title: "Use Spark for EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
```

```{r}
source("SparkFunctions.R")
```

```{r}
source("./Consts.R")
source("./FilePaths.R")
source("./CleanStatsTable.R")
```

Connect to local Spark standalone.

```{r connect-to-local-master}
sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
spark_web(sc)
```

Copy mtcars to cluster.

```{r load-multiple-files-usa}
sdf <- spark_read_csv(
  path = "file:///Users/jawaidhakim/Downloads/CUNY/DATA607/DATA607-FINALPROJECT/data/processed/*/*/usa-*.csv",
  sc = sc, name = "coviddata", header=TRUE, memory = TRUE)
sdf
```

```{r convert-to-dataframe-usa}
df <- sdf_collect(sdf) |> CleanStatsTable.USA() |> CleanDateComponents()
df
```

```{r visualize-col-types-usa}
library(visdat)
df |> 
    sample_n(ifelse(NROW(df) > 5000, 5000, NROW(df))) |> # downsample
    vis_dat(palette = "cb_safe") # color-blind safe 
```


```{r}
library(ggplot2)
```


```{r plot-testing-rate}
p <- ggplot(df, aes(x = Date, y = Testing_Rate, color = Province_State)) + geom_line()
ggplotly(p)
```

```{r plot-incident-rate}
p <- ggplot(df, aes(x = Date, y = Incident_Rate, color = Province_State)) + geom_line()
ggplotly(p)
```

```{r plot-deaths}
# Note: states with higher population will have higher number of deaths
p <- ggplot(df, aes(x = Date, y = Deaths, color = Province_State)) + geom_line()
ggplotly(p)
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
summarize_all(sdf, mean) %>%
  show_query()
```


```{r}
summarize_all(sdf, mean)
```

Run correlation across all numeric variables.

```{r}
#ml_corr(sdf)
```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.ShowJps()
```

