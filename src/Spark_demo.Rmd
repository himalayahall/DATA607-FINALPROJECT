---
title: "Use Spark for EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
library(assertive)
```

```{r}
source("SparkFunctions.R")
```

```{r}
source("./Consts.R")
source("./FilePaths.R")
source("./CleanStatsTable.R")
```

Connect to local Spark standalone.

```{r connect-to-local-master}
sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
spark_web(sc)
```


```{r load-category-csv-files}
GetFileRegexp <- function(dataFreq = '2pm', is_global_data = FALSE, verbose = TRUE) {
    n <- as.integer(str_replace(dataFreq, "pm", ""))
    file_regexp <- switch(
        n,
        c('*-*-15'),
        c('*-*-15', '*-*-25'),
        c('*-*-09', '*-*-18', '*-*-25'),
        c('*-*-05', '*-*-12', '*-*-20', '*-*-27')
    )
    
    
    assert_is_not_null(file_regexp, "stop")
    
    file_prefix <- 'usa'
    if (is_global_data) {
        file_prefix <- 'global'
    }
    
    if (verbose) {
        print(paste0("File regexp: ", file_regexp))
    }
    
    return (paste0(file_prefix, "-", file_regexp))    
}

#
# Load data into Spark.
#
# Parameters:
#   dataFreq : Data frequency: c('1pm', '2pm', '3pm', '4pm') for 1, 2 (default), 3, or 4 samples per month
#   global: TRUE for global dataset, FALSE for USA (default)
LoadDataset <- function(dataFreq = '2pm', global = FALSE, verbose = TRUE) {
    file_regexp <- GetFileRegexp(dataFreq, global, verbose)
    full_sdf <- NULL
    for (r in file_regexp) {
        full_regexp <- paste0(
            "file:///Users/jawaidhakim/Downloads/CUNY/DATA607/DATA607-FINALPROJECT/data/processed/*/*/",
            r,
            ".csv"
        )
        
        if (verbose) {
            print(paste0("Loading data from: ", full_regexp))
        }
        
        sdf <- spark_read_csv(
            full_regexp,
            sc = sc,
            name = "coviddata",
            header = TRUE,
            memory = TRUE
        )
        
        if (is.null(full_sdf)) {
            full_sdf <- sdf
        } else {
            full_sdf = sdf_bind_rows(full_sdf, sdf)
        }
    }
   
    sdf_dim(full_sdf)
    return (full_sdf)
}
```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```


```{r}
sdf <- LoadDataset(dataFreq = '1pm',  global = is_global_data, verbose = TRUE)
```

Mutate column data types

```{r}
if (is_global_data) {
    sdf <- sdf |> CleanStatsTable.Global() |> CleanDateComponents()
} else {
    sdf <- sdf |> CleanStatsTable.USA() |> CleanDateComponents()
}
sdf
```

Collect data in R dataframe

```{r convert-to-dataframe-usa}
df <- sdf_collect(sdf)
df
```


```{r visualize-col-types-usa}
library(visdat)
df |>
    sample_n(ifelse(NROW(df) > 5000, 5000, NROW(df))) |> # downsample
    vis_dat() # color-blind safe 
```


```{r}
library(ggplot2)
library(plotly)
```


```{r plot-testing-rate}
p <- NULL
if (is_global_data) {
    p <-
        ggplot(df, aes(x = Date, y = Testing_Rate, color = Country_Region)) + geom_line()
} else {
    p <-
        ggplot(df, aes(x = Date, y = Testing_Rate, color = Province_State)) + geom_line()
    ggplotly(p)
}
```


```{r plot-deaths}
# Note: states with higher population will have higher number of deaths
p <- NULL
if (is_global_data) {
    p <-
        ggplot(df, aes(x = Date, y = Deaths, color = Country_Region)) + geom_line()
    ggplotly(p)
} else {
    p <-
        ggplot(df, aes(x = Date, y = Deaths, color = Province_State)) + geom_line()
    ggplotly(p)
}
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
df1 <- NULL
if (is_global_data) {
    df1 <- df |>
        filter(!is.na(Lat)) |>
        filter(!is.na(Long_)) |>
        group_by(Country_Region) |>
        slice(which.max(Deaths)) |>
        mutate(TotalDeaths = Deaths) |>
        select(Province_State, Country_Region, Lat, Long_, Deaths, TotalDeaths)
        #distinct(Province_State, Country_Region) |>

} else {
    df1 <- df |>
        filter(!is.na(Lat)) |>
        filter(!is.na(Long_)) |>
        group_by(Province_State) |>
        filter(Deaths == cumsum(Deaths)) |> 
        select(Province_State, Country_Region, Lat, Long_, Deaths) |>
        unique()
}
```

```{r}

max_deaths <- max(df1$TotalDeaths)
df1 <-
    df1 |> 
    mutate(scaled_tot_deaths = TotalDeaths / max_deaths, .after = Deaths) |>
    mutate(across(matches(c("Lat", "Long_")), as.numeric))
    
df1_low <- df1 |> filter(scaled_tot_deaths < 0.5)
df1_high <- df1 |> filter(scaled_tot_deaths >= 0.5
)

```

```{r}
library(leaflet)
library(leaflet.extras)
```


```{r}
m <- NULL
if (is_global_data) {
    m <- leaflet::leaflet(df1) %>%
        leaflet::addProviderTiles("Stamen.Toner") %>%
        leaflet::addCircleMarkers(
            lng = ~ Long_,
            lat = ~ Lat,
            label = ~ paste0(Country_Region, ", ", Province_State, " Deaths: ", Deaths),
            color = 'red',
            radius = ~ scaled_tot_deaths * 10
        ) |>
        leaflet.extras::addResetMapButton()
    
    
} else {
    m <- leaflet::leaflet(df1) %>%
        leaflet::addProviderTiles("Stamen.Toner") %>%
        leaflet::addCircleMarkers(
            lng = ~ Long_,
            lat = ~ Lat,
            label = ~ paste0(Province_State, ", Deaths: ", Deaths),
            color = 'red',
            radius = ~ scaled_tot_deaths * 10
        ) |>
        leaflet.extras::addResetMapButton()
}
m  # Print the map
```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.ShowJps()
```

