---
title: "Use Spark for EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
library(visdat)
```

```{r}
source("SparkFunctions.R")
```

```{r}
source("./Consts.R")
source("./FilePaths.R")
source("./CleanStatsTable.R")
```

Connect to local Spark standalone.

```{r connect-to-local-master}
sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
spark_web(sc)
```

Copy mtcars to cluster.

```{r load-multiple-files-usa}
sdf <- spark_read_csv(
  path = "file:///Users/jawaidhakim/Downloads/CUNY/DATA607/DATA607-FINALPROJECT/data/processed/*/*/usa-*.csv",
  sc = sc, name = "coviddata", header=TRUE, memory = TRUE)
sdf
```

```{r convert-to-dataframe-usa}
df <- sdf |> collect()
df <- CleanStatsTable.USA(df)
df
```

```{r visualize-col-types-usa}
df |> 
    sample_n(ifelse(NROW(df) > 1000, 1000, NROW(df))) |> # downsample
    vis_dat(palette = "cb_safe") # color-blind safe 
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
summarize_all(sdf, mean) %>%
  show_query()
```


```{r}
summarize_all(sdf, mean)
```

Run correlation across all numeric variables.

```{r}
#ml_corr(sdf)
```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.KillCluster()
```

