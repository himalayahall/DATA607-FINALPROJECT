---
title: "Use Spark for EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
```

```{r}
source("SparkFunctions.R")
```

```{r}
source("./Consts.R")
source("./FilePaths.R")
source("./CleanStatsTable.R")
```

Connect to local Spark standalone.

```{r connect-to-local-master}
sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
spark_web(sc)
```


```{r load-all-files-usa}
sdf <- spark_read_csv(
  path = "file:///Users/jawaidhakim/Downloads/CUNY/DATA607/DATA607-FINALPROJECT/data/processed/*/*/usa-*.csv",
  sc = sc, name = "coviddata", header=TRUE, memory = TRUE)
sdf_dim(sdf)
sdf
```

Mutate column data types

```{r}
sdf <- sdf |> CleanStatsTable.USA() |> CleanDateComponents()
sdf
```

Collect data in R dataframe

```{r convert-to-dataframe-usa}
df <- sdf_collect(sdf)
df
```

```{r visualize-col-types-usa}
library(visdat)
df |> 
    sample_n(ifelse(NROW(df) > 5000, 5000, NROW(df))) |> # downsample
    vis_dat(palette = "cb_safe") # color-blind safe 
```


```{r}
library(ggplot2)
```


```{r plot-testing-rate}
p <- ggplot(df, aes(x = Date, y = Testing_Rate, color = Province_State)) + geom_line()
ggplotly(p)
```

```{r plot-incident-rate}
p <- ggplot(df, aes(x = Date, y = Incident_Rate, color = Province_State)) + geom_line()
ggplotly(p)
```

```{r plot-deaths}
# Note: states with higher population will have higher number of deaths
p <- ggplot(df, aes(x = Date, y = Deaths, color = Province_State)) + geom_line()
ggplotly(p)
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
summarize_all(sdf, mean) %>%
  show_query()
```

Run correlation across all numeric variables.

```{r}
sdf <- sdf |> CleanStatsTable.USA() |> CleanDateComponents()
sparklyr::
```

```{r}
# library(magrittr)
# library(maps)
# library(mapproj)
# library(viridis)

```


```{r}
 df1 <- df |>
    filter(! is.na(Lat)) |>
    filter(! is.na(Long_)) |>
    group_by(Province_State) |>
    filter(Deaths == max(Deaths)) |>     # pick out greatest death count
    select(Province_State, Lat, Long_, Deaths) |>
    unique()

max_deaths <- max(df1$Deaths)
df1 <- df1 |> mutate(scaled_tot_deaths = Deaths / max_deaths, .after = Deaths)
df1_low <- df1 |> filter(scaled_tot_deaths < 0.5)
df1_high <- df1 |> filter(scaled_tot_deaths >= 0.5)
```


```{r}
library(leaflet)
library(leaflet.extras)
```


```{r}
m <- leaflet::leaflet(df1) %>%
    leaflet::addProviderTiles("Stamen.Toner") %>%
    leaflet::addCircleMarkers(
        lng = ~ Long_,
        lat = ~ Lat,
        label = ~ paste0(Province_State, ", Deaths: ", Deaths),
        color = 'red',
        radius = ~ scaled_tot_deaths * 10
    ) |>
      leaflet.extras::addResetMapButton()
    
m  # Print the map
```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.ShowJps()
```

