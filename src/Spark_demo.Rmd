---
title: "Use Spark for EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
```

```{r}
source("SparkFunctions.R")
```

```{r}
source("./Consts.R")
source("./FilePaths.R")
```

Connect to local Spark standalone.

```{r connect-to-local-master}
sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
spark_web(sc)
```

Copy mtcars to cluster.

```{r load-multiple-files}
spark_read_csv(
  path = "file:///Users/jawaidhakim/Downloads/CUNY/DATA607/DATA607-FINALPROJECT/data/processed/*/*/*.csv",
  sc = sc, name = "coviddata", header=TRUE, memory = TRUE)
```

```{r}
spark_read_table(sc, name='coviddata')
```

```{r}
select(df, Province_State, Date, Year, Month, Day, Confirmed, Deaths, Recovered, Active, Incident_Rate, People_Hospitalized, Case_Fatality_Ratio, Testing_Rate, Hospitalization_Rate) %>%
  sample_n(100) %>%
  collect() #%>%
  #plot()
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
summarize_all(df, mean) %>%
  show_query()
```


```{r}
summarize_all(df, mean)
```

Run correlation across all numeric variables.

```{r}
# ml_corr(df)
```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.KillCluster()
```

