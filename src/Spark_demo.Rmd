---
title: "Use Spark for EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
library(assertive)
library(readr)
```

```{r}
source("SparkFunctions.R")
```

```{r}
source("./Consts.R")
source("./FilePaths.R")
source("./CleanStatsTable.R")
```

Connect to local Spark standalone.

```{r connect-to-local-master}
sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
spark_web(sc)
```


```{r load-into-spark}
#
# Load time-series data into Spark.
#
# Parameters:
#   ts_type: c('deaths', 'confirmed')
#   keep_latest: TRUE to keep only last time series observation, FALSE to keep all
#   global: TRUE for global dataset, FALSE for USA (default)
LoadTimeSeriesDataset <- function(ts_type, keep_latest, global = FALSE, verbose = TRUE) {
    file_name <-
        Output.GetTimeSeriesFileName(ts_type = ts_type, global = global)
    
    cwd <- getwd()
    
    # Province/State,Country/Region,Lat,Long
    
    # ID,iso2,iso3,code3,FIPS,Admin2,Province_State,Country_Region,Lat,Long_,Combined_Key
    file_path <-
        paste0(cwd, "/../data/processed/timeseries/", file_name)
    df <- readr::read_csv(file = file_path, col_names = TRUE)
    
    # remove backslash from character fields
    df <- rename(df, 'Province_State' = 'Province/State')
    df <- rename(df, 'Country_Region' = 'Country/Region')
    
    # pivot longer on date columns
    df <- df |>
        pivot_longer(contains('/'), names_to = 'date', values_to = ts_type)
    
    # mutate date type
    df <- df |>
        mutate(date = as.Date(date, format = '%m/%d/%y'))
    
    if (keep_latest) {
        df <- df |> group_by(Country_Region, Province_State) |> filter(row_number() == n())
    }

    max_deaths <- max(df$deaths)
    df <- df |> 
        mutate(scaled_tot_deaths = deaths / max_deaths)

    sdf <- copy_to(sc,
                   df,
                   name = "coviddata",
                   header = TRUE,
                   memory = TRUE,
                   overwrite = TRUE)
    sdf_dim(sdf)
    return (sdf)
}

GetDailyFileRegexp <- function(dataFreq = '2pm', is_global_data = FALSE, verbose = TRUE) {
    n <- as.integer(str_replace(dataFreq, "pm", ""))
    file_regexp <- switch(
        n,
        c('*-*-15'),
        c('*-*-15', '*-*-25'),
        c('*-*-09', '*-*-18', '*-*-25'),
        c('*-*-05', '*-*-12', '*-*-20', '*-*-27')
    )
    
    
    assert_is_not_null(file_regexp, "stop")
    
    file_prefix <- 'usa'
    if (is_global_data) {
        file_prefix <- 'global'
    }
    
    if (verbose) {
        print(paste0("File regexp: ", file_regexp))
    }
    
    return (paste0(file_prefix, "-", file_regexp))    
}

#
# Load daily data into Spark.
#
# Parameters:
#   dataFreq : Data frequency: c('1pm', '2pm', '3pm', '4pm') for 1, 2 (default), 3, or 4 samples per month
#   global: TRUE for global dataset, FALSE for USA (default)
LoadDailyDataset <- function(dataFreq = '2pm', global = FALSE, verbose = TRUE) {
    file_regexp <- GetDailyFileRegexp(dataFreq, global, verbose)
    full_sdf <- NULL
    cwd <- getwd()
    for (r in file_regexp) {
        full_regexp <- paste0("file:///", cwd, "../data/processed/*/*/",
                              r,
                              ".csv")
        
        if (verbose) {
            print(paste0("Loading data from: ", full_regexp))
        }
        
        sdf <- spark_read_csv(
            full_regexp,
            sc = sc,
            name = "coviddata",
            header = TRUE,
            memory = TRUE
        )
        
        if (is.null(full_sdf)) {
            full_sdf <- sdf
        } else {
            full_sdf = sdf_bind_rows(full_sdf, sdf)
        }
    }
    
    sdf_dim(full_sdf)
    return (full_sdf)
}
```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```


```{r load-daily-dataset}
#sdf <- LoadDailyDataset(dataFreq = '1pm',  global = is_global_data, verbose = TRUE)
```


```{r load-ts-dataset}
sdf <- LoadTimeSeriesDataset(ts_type = 'deaths', keep_latest = TRUE, global = TRUE, verbose = TRUE)
sdf
```

Mutate column data types

```{r mutate-daily-dataset}
# if (is_global_data) {
#     sdf <- sdf |> CleanStatsTable.Global() |> CleanDateComponents()
# } else {
#     sdf <- sdf |> CleanStatsTable.USA() |> CleanDateComponents()
# }
# sdf
```


```{r visualize-col-types-usa}
# library(visdat)
# df <- sdf_collect(sdf)
# df |>
#     sample_n(ifelse(NROW(df) > 5000, 5000, NROW(df))) |> # downsample
#     vis_dat() # color-blind safe 
```


```{r}
library(ggplot2)
library(plotly)
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
# max_deaths <- max(df$deaths)
# df1 <-
#     df |> 
#     mutate(scaled_tot_deaths = deaths / max_deaths, .after = deaths)
# 
# df1_low <- df1 |> filter(scaled_tot_deaths < 0.5)
# df1_high <- df1 |> filter(scaled_tot_deaths >= 0.5

```

```{r}
library(leaflet)
library(leaflet.extras)
library(leaflet.providers)
```


```{r}

# Have not investigated using Leaflet directly with Spark (does not work on first try), we have create R dataframe
system.time(df1 <-
                sdf_collect(sdf))    # collect Speak dataframe ion R dataframe

m <- NULL
if (is_global_data) {
    leaflet_map <- leaflet::leaflet(df1) %>%
        #leaflet::addTiles() |>
        #addProviderTiles(providers$Stamen.Toner) |>
        #addProviderTiles(providers$CartoDB.Positron) |>
        #addProviderTiles(providers$Esri.NatGeoWorldMap, options = providerTileOptions(opacity = 0.85)) |>
        addProviderTiles(providers$Stamen.TonerLite) |>
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Country_Region, ", ", Province_State, " Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_tot_deaths * 15
        ) |>
        leaflet.extras::addResetMapButton()
} else {
    leaflet_map <- leaflet::leaflet(df1) %>%
        leaflet::addProviderTiles("Stamen.Toner") %>%
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Province_State, ", Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_tot_deaths * 10
        ) |>
        leaflet.extras::addResetMapButton()
}
leaflet_map  # Print the map
```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.ShowJPS()
```

