---
title: "Use Spark for daily data EDA"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Local Spark installation

There are excellent cloud hosted Spark services but are either not free or time limited. A local Spark installation is great for dev/test. You will need to install Java, Python, PySpark, and (optional) Scala. Here are [instructions](https://sparkbyexamples.com/pyspark) on how to install Spark on your local machine.

# Load library

```{r}
library(tidyverse)
library(sparklyr)
library(DBI)
library(assertive)
library(readr)
```

```{r}
source("SparkFunctions.R")
source("Consts.R")
source("FilePaths.R")
```

```{r}
source("./DailyData_CleanStatsTable.R")
```

Connect to local Spark standalone.

```{r connect-to-local-master}
sc <- Spark.ConnectLocal()
```

Display web console.

```{r display-web-console}
spark_web(sc)
```


```{r load-into-spark}
GetDailyFileRegexp <- function(dataFreq = '2pm', is_global_data = FALSE, verbose = TRUE) {
    n <- as.integer(str_replace(dataFreq, "pm", ""))
    file_regexp <- switch(
        n,
        c('*-*-15'),
        c('*-*-15', '*-*-25'),
        c('*-*-09', '*-*-18', '*-*-25'),
        c('*-*-05', '*-*-12', '*-*-20', '*-*-27')
    )
    
    
    assert_is_not_null(file_regexp, "stop")
    
    file_prefix <- 'usa'
    if (is_global_data) {
        file_prefix <- 'global'
    }
    
    if (verbose) {
        print(paste0("File regexp: ", file_regexp))
    }
    
    return (paste0(file_prefix, "-", file_regexp))    
}

#
# Load daily data into Spark.
#
# Parameters:
#   dataFreq : Data frequency: c('1pm', '2pm', '3pm', '4pm') for 1, 2 (default), 3, or 4 samples per month
#   global: TRUE for global dataset, FALSE for USA (default)
LoadDailyDataset <- function(dataFreq = '2pm', global = FALSE, verbose = TRUE) {
    file_regexp <- GetDailyFileRegexp(dataFreq, global, verbose)
    full_sdf <- NULL
    cwd <- getwd()
    for (r in file_regexp) {
        full_regexp <- paste0("file:///", cwd, "../data/processed/*/*/",
                              r,
                              ".csv")
        
        if (verbose) {
            print(paste0("Loading data from: ", full_regexp))
        }
        
        sdf <- spark_read_csv(
            full_regexp,
            sc = sc,
            name = "coviddata",
            header = TRUE,
            memory = TRUE
        )
        
        if (is.null(full_sdf)) {
            full_sdf <- sdf
        } else {
            full_sdf = sdf_bind_rows(full_sdf, sdf)
        }
    }
    
    sdf_dim(full_sdf)
    return (full_sdf)
}
```

Select between USA or Global data

```{r define-data-category}
is_global_data <- TRUE
```


```{r load-daily-dataset}
sdf <- LoadDailyDataset(dataFreq = '1pm',  global = is_global_data, verbose = TRUE)
sdf
```


Mutate column data types

```{r mutate-daily-dataset}
if (is_global_data) {
    sdf <- sdf |> CleanDDStatsTable.Global() |> CleanDateComponents()
} else {
    sdf <- sdf |> CleanDDStatsTable.USA() |> CleanDateComponents()
}
sdf
```


```{r visualize-col-types-usa}
library(visdat)
df <- sdf_collect(sdf)
df |>
    sample_n(ifelse(NROW(df) > 5000, 5000, NROW(df))) |> # downsample
    vis_dat() # color-blind safe
```


```{r}
library(ggplot2)
library(plotly)
```

Run usual R workflows against dataset, no change to syntax but data is in cluster.

```{r}
max_deaths <- max(df$deaths)
df1 <-
     df |> 
     mutate(scaled_tot_deaths = deaths / max_deaths, .after = deaths)
```

```{r}
library(leaflet)
library(leaflet.extras)
library(leaflet.providers)
```


```{r}

# Have not investigated using Leaflet directly with Spark (does not work on first try), we have create R dataframe
system.time(df1 <-
                sdf_collect(sdf))    # collect Speak dataframe ion R dataframe

m <- NULL
if (is_global_data) {
    leaflet_map <- leaflet::leaflet(df1) %>%
        #leaflet::addTiles() |>
        #addProviderTiles(providers$Stamen.Toner) |>
        #addProviderTiles(providers$CartoDB.Positron) |>
        #addProviderTiles(providers$Esri.NatGeoWorldMap, options = providerTileOptions(opacity = 0.85)) |>
        addProviderTiles(providers$Stamen.TonerLite) |>
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Country_Region, ", ", Province_State, " Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_var * 15
        ) |>
        leaflet.extras::addResetMapButton()
} else {
    leaflet_map <- leaflet::leaflet(df1) %>%
        leaflet::addProviderTiles("Stamen.Toner") %>%
        leaflet::addCircleMarkers(
            lng = ~ Long,
            lat = ~ Lat,
            label = ~ paste0(Province_State, ", Deaths: ", deaths),
            color = 'red',
            radius = ~ scaled_tot_deaths * 10
        ) |>
        leaflet.extras::addResetMapButton()
}
leaflet_map  # Print the map
```

Disconnect from cluster.

```{r}
Spark.Disconnect(sc)
```

```{r}
Spark.ShowJPS()
```

