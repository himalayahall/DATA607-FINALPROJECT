---
title: "Scrape Novel Coronavirus (COVID-19) Cases, provided by JHU CSSE"
author: "Jawaid Hakim"
date: "`r Sys.Date()`"
output:
  
  html_document:
    
    toc: true
    toc_float: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
boxlinks: true
urlcolor: blue
always_allow_html: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load library

```{r}
library(RSelenium)
library(tidyverse)
library(rvest)
library(lubridate)
```

# Selenium setup

Start a selenium server and browser
```{r}
port <- 4545L
rsDrv <- rsDriver(browser="firefox", port=port, verbose=F)

# Sleep to allow driver time to launch headless browser
Sys.sleep(5)
```

Grab  reference to remote client driver
```{r}
rsClient <- rsDrv[["client"]]
```

# Generate list of data to be processed

Navigate to base URL.

```{r}
rsClient$navigate("https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us")
Sys.sleep(2)
```

Find elements.

```{r}
css_selector <- "a.js-navigation-open.Link--primary"
elems <- rsClient$findElements(using = "css selector", 
                               value = css_selector)
```

Create tibble with 2 columns - name of CSV file name and HREF.

```{r}
tbl_href <- tibble(name = character(), href = character())
for (elem in elems) {
    name <- as.character(elem$getElementText())
    if (tolower(name) != "readme.md") {
        href = as.character(elem$getElementAttribute("href"))
        tbl_href <- tbl_href %>% add_row(name = name, href = href)
    }
}
head(tbl_href, n = 5)
```

Stop selenium server.
```{r}
rsDrv$server$stop()
```


Add date column.

```{r}
tbl_href <- tbl_href %>% 
    mutate(sdate = str_remove(name, ".csv"), 
           date = as.Date(sdate, "%m-%d-%Y")) %>%
    select(-sdate)

head(tbl_href, n = 5)
```

Extract year, month, day into separate columns.
```{r}
tbl_href <- tbl_href %>% 
    mutate(year = lubridate::year(date),
           month = lubridate::month(date), 
           day = lubridate::day(date)) %>%
    mutate(year = as.integer(year),
           month = as.integer(month),
           day = as.integer(day))

head(tbl_href, n = 5)
```

# Setup parallel processing

Function run on cluster nodes to process one data item (URL).

```{r par-usa-stats}

ScrapeStats_USA <- function(...) {
    
    # need to load libraries on cluster node
    library(tidyverse)
    library(rvest)
    library(lubridate)
    
    # get index of URL to process
    scraped_url_index <- (...)
    
    usa_stats_css_selector <- ".js-csv-data"
    
    # get URL from exported namespace
    targetUrl <- tbl_href$href[scraped_url_index]
    
    # create traget URL
    if (! str_starts(targetUrl, "https:")) {
        if (str_starts(targetUrl, "/")) {
            targetUrl <- paste0("https://github.com", targetUrl)
        }
        else {
            targetUrl <- paste0("https://github.com/", targetUrl)
        }
    }
    
    # read from target URL, grab table data
    stats_table <- read_html(targetUrl) %>% 
        html_element(css = usa_stats_css_selector) %>%
        html_table()
    
    # first row contains column names
    first_row <- stats_table[1, ]
    colnames(stats_table) <- first_row[1,]
    
    # 
    stats_table <- stats_table %>% 
        select(-'NA') %>%    # first col is named 'NA', remove it
        slice(-1) %>%        # first row contains column names, remove it
        transmute(Confirmed = as.integer(Confirmed),
               Deaths = as.integer(Deaths),
               Recovered = as.integer(Recovered),
               Active = as.integer(Active),
               FIPS = as.numeric(FIPS),
               People_Hospitalized = as.integer(People_Hospitalized),
               Case_Fatality_Ratio = as.numeric(Case_Fatality_Ratio),
               UID = as.integer(UID),
               
               Date = as.Date(Date),
               Last_Update = as_datetime(Last_Update),
               
               People_Tested = as.integer(People_Tested),
               
               Hospitalization_rate = as.numeric(Hospitalization_Rate),
               Mortality_Rate = as.numeric(Mortality_Rate),
               Testing_Rate = as.numeric(Testing_Rate),
               Incident_Rate = as.numeric(Incident_Rate),
               Total_Test_Results = as.integer(Total_Test_Results),
        )
    
    return (stats_table)
}
```

Load parallel processing libraries
```{r par-load-library}
library(doParallel)
library(parallel)
```

Detect cores.

```{r par-num-cores}
num_cores <- detectCores(logical = TRUE)
num_cores
```

Create functions to start/stop local cluster

```{r}
StartCluster <- function(verbose = TRUE) {
    num_cluster_nodes <- 1 + (num_cores / 2)
    cluster <- makeCluster(num_cluster_nodes)
    registerDoParallel(cluster)
    print(paste0("Started cluster with ", num_cluster_nodes, " nodes"))
    return (cluster)
}

StopCluster <- function(verbose = TRUE) {
    stopCluster(cluster)
    print("Stopped cluster")
}

ExportNamespaceToCluster <- function(namespace) {
    clusterExport(cluster, namespace) #list('ScrapeStats_USA', 'tbl_href'))
}
```

Start cluster
```{r}
cluster <- StartCluster()
```

Export namespace to cluster nodes
```{r}
ExportNamespaceToCluster(list('ScrapeStats_USA', 'tbl_href'))
```

Generate sequence id for all URLs. Each work item - i.e. URL to be processed - will be indentified by it's index in `tbl_href` table.

```{r par-generate-url-id}
seq_id_all <- seq_along(1:NROW(tbl_href))

# For dev/testing purposes create a small sequence. For example, a sequence of 1:5 will process first 5 URLs
# For the final run use seq_along(1:NROW(tbl_href))
seq_id_all <- seq_along(1:5)
glimpse(seq_id_all)
```

Process all URLs using Map/Reduce
```{r par-process}
system.time(
    work_results <- c(parLapply(cluster, seq_id_all, fun=ScrapeStats_USA))
)
```

Cluster has finished it's work. Stop cluster
```{r}
StopCluster()

```

Combine nested results

```{r}
cum_results <- bind_rows(work_results)
glimpse(cum_results)
```

# Spark

There are many cloud hosted Spark deployments - e.g. [Databricks Apache Spark](https://www.databricks.com/spark/about) and [AWS EM](https://aws.amazon.com/emr/features/spark/) - but they have very  time-limited free tiers.  

However, we can leverage a standalone Spark cluster as a proof-of-concept. The standalone version provides all the functionality we need to dp Spark based EDAs.

```{r}
#library(sparklyr)

# Install standalone Spark
# spark_install("3.3.2)
```

Load helper Spark functions.

```{r}
source("./SparkFunctions.R")
```

Connect to standalone cluster.
```{r}
sc <- Spark.ConnectLocal()
```

Launch Web console to cluster.
```{r}
Spark.Web(sc)
```

Copy dataset to Spark cluster.
```{r}
cum_results <- copy_to(sc, cum_results, overwrite = TRUE)
```

Look at dataset. Notice, it is loaded from the cluster.
```{r}
glimpse(cum_results)
```

Disconnect from cluster.
```{r}
Spark.Disconnect(sc)
```

